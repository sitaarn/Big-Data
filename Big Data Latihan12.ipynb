{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f18e5bb-51a0-4578-9707-75bebb04d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "      Name    Age   Salary\n",
      "0    Alice  24.00  48000.0\n",
      "1      Bob  30.00  57000.0\n",
      "2  Charlie  27.75  57000.0\n",
      "3    David  22.00  57000.0\n",
      "4     None  35.00  60000.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Identifying and Handling Missing Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', None],\n",
    "    'Age': [24, 30, None, 22, 35],\n",
    "    'Salary': [48000, None, 57000, None, 60000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filling missing values and dropping rows\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].median())\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596ea0f8-4928-4bf8-bb78-212a6d35fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      "    Product     Category\n",
      "0   Laptop  Electronics\n",
      "1   Laptop  Electronics\n",
      "2  Desktop  Electronics\n",
      "3   Tablet      Gadgets\n",
      "4   Tablet      Gadgets\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Standardizing Categorical Data\n",
    "# Sample dataset with inconsistent categorical values\n",
    "\n",
    "data = {\n",
    "    'Product': ['Laptop', 'Laptop', 'Desktop', 'Tablet', 'Tablet'],\n",
    "    'Category': ['Electronics', 'electronics', 'Electronics', 'Gadgets', 'gadgets']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize category values\n",
    "df['Category'] = df['Category'].str.capitalize()\n",
    "print('Standardized Data:\\n', df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185184c0-2162-446e-afef-184835afe8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset berhasil dimuat dari: /home/sita/Downloads/titanic.csv\n",
      "\n",
      "Preview Data:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0          892         0       3   \n",
      "1          893         1       3   \n",
      "2          894         0       2   \n",
      "3          895         0       3   \n",
      "4          896         1       3   \n",
      "\n",
      "                                           Name     Sex   Age  SibSp  Parch  \\\n",
      "0                              Kelly, Mr. James    male  34.5      0      0   \n",
      "1              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
      "2                     Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
      "3                              Wirz, Mr. Albert    male  27.0      0      0   \n",
      "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
      "\n",
      "    Ticket     Fare Cabin Embarked  \n",
      "0   330911   7.8292   NaN        Q  \n",
      "1   363272   7.0000   NaN        S  \n",
      "2   240276   9.6875   NaN        Q  \n",
      "3   315154   8.6625   NaN        S  \n",
      "4  3101298  12.2875   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"/home/sita/Downloads/titanic.csv\"\n",
    "\n",
    "# Baca dataset Titanic\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"Dataset berhasil dimuat dari:\", path)\n",
    "print(\"\\nPreview Data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d0482c-ed65-4cdc-8cc6-23bca2c0ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Informasi Dataset ===\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Survived     418 non-null    int64  \n",
      " 2   Pclass       418 non-null    int64  \n",
      " 3   Name         418 non-null    object \n",
      " 4   Sex          418 non-null    object \n",
      " 5   Age          332 non-null    float64\n",
      " 6   SibSp        418 non-null    int64  \n",
      " 7   Parch        418 non-null    int64  \n",
      " 8   Ticket       418 non-null    object \n",
      " 9   Fare         417 non-null    float64\n",
      " 10  Cabin        91 non-null     object \n",
      " 11  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 39.3+ KB\n",
      "None\n",
      "\n",
      "=== Missing Values ===\n",
      "\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Practice Task 1: Load a dataset of your choice and identify missing values.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "path = \"/home/sita/Downloads/titanic.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Cek informasi umum\n",
    "print(\"=== Informasi Dataset ===\\n\")\n",
    "print(df.info())\n",
    "\n",
    "# Cek jumlah missing value per kolom\n",
    "print(\"\\n=== Missing Values ===\\n\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987defa7-d76a-4158-b222-ec0a06a76093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.8.0\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m850.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cb8532-ffc9-4a0a-87c7-73bffc1d906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data setelah normalisasi (7 baris) ===\n",
      "\n",
      "   PassengerId  Survived  Pclass       Age  SibSp     Parch      Fare\n",
      "0     0.000000       0.0     1.0  0.452723  0.000  0.000000  0.015282\n",
      "1     0.002398       1.0     1.0  0.617566  0.125  0.000000  0.013663\n",
      "2     0.004796       0.0     0.5  0.815377  0.000  0.000000  0.018909\n",
      "3     0.007194       0.0     1.0  0.353818  0.000  0.000000  0.016908\n",
      "4     0.009592       1.0     1.0  0.287881  0.125  0.111111  0.023984\n",
      "5     0.011990       0.0     1.0  0.182382  0.000  0.000000  0.018006\n",
      "6     0.014388       1.0     1.0  0.393380  0.000  0.000000  0.014891\n"
     ]
    }
   ],
   "source": [
    "# Practice Task 2: Implement data transformations to normalize numerical columns.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Pilih kolom numerik\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Buat salinan data untuk normalisasi\n",
    "df_norm = df.copy()\n",
    "\n",
    "# Terapkan normalisasi Min-Max\n",
    "scaler = MinMaxScaler()\n",
    "df_norm[numeric_cols] = scaler.fit_transform(df_norm[numeric_cols])\n",
    "\n",
    "print(\"\\n=== Data setelah normalisasi (7 baris) ===\\n\")\n",
    "print(df_norm[numeric_cols].head(7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03f410f-4534-4fe6-b18f-125bc5dbc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jumlah data sebelum hapus duplikat: 418\n",
      "Jumlah data setelah hapus duplikat: 418\n",
      "\n",
      "=== Contoh data setelah encoding ===\n",
      "\n",
      "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  \\\n",
      "0          892         0       3  34.5      0      0   7.8292   \n",
      "1          893         1       3  47.0      1      0   7.0000   \n",
      "2          894         0       2  62.0      0      0   9.6875   \n",
      "3          895         0       3  27.0      0      0   8.6625   \n",
      "4          896         1       3  22.0      1      1  12.2875   \n",
      "\n",
      "   Name_Abelseth, Miss. Karen Marie  Name_Abelseth, Mr. Olaus Jorgensen  \\\n",
      "0                             False                               False   \n",
      "1                             False                               False   \n",
      "2                             False                               False   \n",
      "3                             False                               False   \n",
      "4                             False                               False   \n",
      "\n",
      "   Name_Abrahamsson, Mr. Abraham August Johannes  ...  Cabin_F  Cabin_F E46  \\\n",
      "0                                          False  ...    False        False   \n",
      "1                                          False  ...    False        False   \n",
      "2                                          False  ...    False        False   \n",
      "3                                          False  ...    False        False   \n",
      "4                                          False  ...    False        False   \n",
      "\n",
      "   Cabin_F E57  Cabin_F G63  Cabin_F2  Cabin_F33  Cabin_F4  Cabin_G6  \\\n",
      "0        False        False     False      False     False     False   \n",
      "1        False        False     False      False     False     False   \n",
      "2        False        False     False      False     False     False   \n",
      "3        False        False     False      False     False     False   \n",
      "4        False        False     False      False     False     False   \n",
      "\n",
      "   Embarked_Q  Embarked_S  \n",
      "0        True       False  \n",
      "1       False        True  \n",
      "2        True       False  \n",
      "3       False        True  \n",
      "4       False        True  \n",
      "\n",
      "[5 rows x 864 columns]\n"
     ]
    }
   ],
   "source": [
    "# Practice Task 3: Standardize categorical columns and remove duplicates.\n",
    "\n",
    "# Pilih kolom kategorikal\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Lakukan one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Hapus duplikat\n",
    "before = len(df_encoded)\n",
    "df_encoded.drop_duplicates(inplace=True)\n",
    "after = len(df_encoded)\n",
    "\n",
    "print(f\"\\nJumlah data sebelum hapus duplikat: {before}\")\n",
    "print(f\"Jumlah data setelah hapus duplikat: {after}\")\n",
    "\n",
    "print(\"\\n=== Contoh data setelah encoding ===\\n\")\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f7bcd-29d6-4852-bce6-0817c7b2a7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (spark310)",
   "language": "python",
   "name": "spark310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
